{
  "name": "Bluestore SMR Support",
  "tagline": "Google Summer of Code 2016",
  "body": "## Objective.\r\nThe Project targets supporting SMR drives for Ceph's Bluestore File System. SMR Drives support restricted access (sequential writes) across a zone. The objective of the project was to make the Bluestore File System capable of writing to SMR Drives.\r\n\r\n## SMR Drives\r\nSMR Drives are of three types: Drive Managed, Host Aware and Host Managed. Drive Managed drives do not expose the internal zone configuration to the host file system. On the other hand, Host Aware and Host Managed drives expose their internal zone layout to the Host layer. A Host Managed SMR drive would not handle out of order writes within a zone. A Host Aware drive will manage out of order writes. This work was done on Host Aware SMR Drive, so even if we ended up doing out of order writes within a zone, the writes would get internally synchronized.\r\n\r\n## Implementation.\r\n\r\n### Benchmarking SMR Drives\r\nHGST Created a libzbc library to benchmark Host Managed and Host Aware SMR drives. I implemented few benchmaking scripts that can be used to benchmark SMR drives [PULL 9534](https://github.com/ceph/ceph/pull/9534)\r\n\r\n### integrating libzbc library\r\nlibzbc library uses AutoMake by default. Since ceph has now migrated to CMakeLists, I added CMake to libzbc here [PULL 2](https://github.com/ceph/libzbc/pull/2)\r\n\r\n### device query code to Allocator\r\n### changes in Freelist Manager\r\n\r\n## TODO\r\n### Add support for ordered thread writes\r\n### Add cleaner / garbage collector\r\n\r\n## Contributors\r\n\r\nThis project is currently maintained by @shehbazj. A big thank you to mentor @liewegas and collaborators @allensamuels @chhabaramesh @alimaredia @badone who helped at different stages of the project.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}